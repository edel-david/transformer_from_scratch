{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from typing import Union,Callable,Optional\n",
    "from layers import Linear,Dropout,Softmax,MLP,LayerNorm\n",
    "from utilsco import compress_numpy_array,decompress_numpy_array\n",
    "from numpy.typing import ArrayLike\n",
    "import math\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,T,C = 2,16,24\n",
    "fake_upst = np.random.randint(-10,10,(B,T,C))\n",
    "t_fake_upstr = torch.tensor(fake_upst,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA\n",
    "\n",
    "class MultiHeadAttention:\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        context_size: int,\n",
    "        n_heads: int,\n",
    "        batch_size: int,\n",
    "        lr: float = 0.1,\n",
    "        dropout: float = 0.1,\n",
    "        c_attn_weight_init_func: Union[Callable, None] = None,\n",
    "        c_proj_weight_init_func: Union[Callable, None] = None,\n",
    "        bias_init_func: Union[Callable, None] = None,\n",
    "    ) -> None:\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.context_size = context_size\n",
    "        self.n_heads = n_heads\n",
    "        self.scale = math.sqrt(d_model)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.attn_dropout = Dropout(dropout)\n",
    "        self.resid_dropout = Dropout(dropout)\n",
    "        self.softmax_attn = Softmax(axis=-1)\n",
    "\n",
    "        if d_model % n_heads != 0:\n",
    "            raise ValueError(\"d_model must be divisible by n_heads\")\n",
    "\n",
    "        self.depth = d_model // n_heads\n",
    "\n",
    "        self.c_attn = Linear(\n",
    "            d_model,\n",
    "            3 * d_model,\n",
    "            batch_size,\n",
    "            lr,\n",
    "            weight_init_func=c_attn_weight_init_func,\n",
    "            bias_init_func=bias_init_func,\n",
    "        )\n",
    "\n",
    "        self.c_proj = Linear(\n",
    "            d_model,\n",
    "            d_model,\n",
    "            batch_size,\n",
    "            lr,\n",
    "            weight_init_func=c_proj_weight_init_func,\n",
    "            bias_init_func=bias_init_func,\n",
    "        )\n",
    "\n",
    "        self.mask = cp.tril(\n",
    "            cp.ones((context_size, context_size), dtype=cp.float64)\n",
    "        ).reshape(1, 1, context_size, context_size)\n",
    "\n",
    "        self.input = None\n",
    "        self.v = None\n",
    "        self.q = None\n",
    "        self.k = None\n",
    "        self.attn = None\n",
    "\n",
    "    def forward(self, input: ArrayLike,train:bool) -> tuple:\n",
    "\n",
    "        self.input = cp.asanyarray(input)\n",
    "\n",
    "        B, T, C = self.input.shape\n",
    "\n",
    "        q, k, v = cp.split(self.c_attn.forward(self.input), 3, axis=2)\n",
    "\n",
    "        k = k.reshape((B, T, self.n_heads, C // self.n_heads)).transpose(\n",
    "            0, 2, 1, 3\n",
    "        )  # (B, nh, T, hs)\n",
    "        q = q.reshape((B, T, self.n_heads, C // self.n_heads)).transpose(\n",
    "            0, 2, 1, 3\n",
    "        )  # (B, nh, T, hs)\n",
    "        v = v.reshape((B, T, self.n_heads, C // self.n_heads)).transpose(\n",
    "            0, 2, 1, 3\n",
    "        )  # (B, nh, T, hs)\n",
    "\n",
    "        # this works because we use reduced dimensionalality with mutlihead attention,\n",
    "        # making it similar in complexity to single attention.\n",
    "\n",
    "        self.k = k\n",
    "        self.q = q\n",
    "        self.v = v\n",
    "\n",
    "        attn = (q @ k.transpose(0, 1, 3, 2)) * (1.0 / math.sqrt(k.shape[-1]))\n",
    "        # k.shape[-1] == C // self.n_heads == multi_head_attention_head_dim == depth\n",
    "\n",
    "        attn = cp.where(self.mask == 0, -1e9, attn)\n",
    "        attn = self.softmax_attn.forward(attn)\n",
    "        attn = self.attn_dropout.forward(attn,train)\n",
    "\n",
    "        self.attn = attn  # 16 x 6 x 256 x 256\n",
    "        # v.shape: 16 x 6 x 256 x 64\n",
    "        x = attn @ v  # x: 16 x 6 x 256 x 64\n",
    "\n",
    "        x = (\n",
    "            cp.ascontiguousarray(x)\n",
    "            .transpose(0, 2, 1, 3)\n",
    "            .reshape(B, -1, self.n_heads * self.depth)\n",
    "        )\n",
    "        x = self.c_proj.forward(x)  # keeps dims\n",
    "        x = self.resid_dropout.forward(x,train)\n",
    "\n",
    "        return x, attn\n",
    "\n",
    "    def backward(self, grad: ArrayLike) -> cp.ndarray:\n",
    "        # grad: 16 x 256 x ...\n",
    "        \n",
    "        B, T, C = self.input.shape\n",
    "        grad = self.resid_dropout.backward(grad)\n",
    "        grad = self.c_proj.backward(grad)\n",
    "        grad = grad.reshape(\n",
    "            (B, T, self.n_heads, self.depth)\n",
    "        ).transpose(\n",
    "            0, 2, 1, 3\n",
    "        )\n",
    "\n",
    "        v_grad2 = self.attn.transpose(0, 1, 3, 2) @ grad\n",
    "        # long_grad is gradient for self.attn\n",
    "        long_grad = grad @ self.v.transpose(0, 1, 3, 2)# long_grad: 16 x 6 x 256 x 64\n",
    "        # v.shape: 16 x 6 x 256 x 64\n",
    "        long_grad = self.attn_dropout.backward(long_grad)\n",
    "        long_grad = self.softmax_attn.backward(long_grad)\n",
    "        long_grad = cp.where(self.mask == 0, 0, long_grad)\n",
    "\n",
    "        long_grad = long_grad * (1 / cp.sqrt(self.depth))\n",
    "        q_grad = long_grad @ self.k  # insert dimensions swaps\n",
    "        k_grad = long_grad.transpose(0, 1, 3, 2) @ self.q  #\n",
    "    \n",
    "\n",
    "        grad = cp.concatenate(\n",
    "            (\n",
    "                q_grad.transpose(0, 2, 1, 3).reshape((B, T, C)),\n",
    "                k_grad.transpose(0, 2, 1, 3).reshape((B, T, C)),\n",
    "                v_grad2.transpose(0, 2, 1, 3).reshape((B, T, C)),\n",
    "            ),\n",
    "            2,\n",
    "        )\n",
    "        down = self.c_attn.backward(grad)\n",
    "        return down\n",
    "\n",
    "    def update(self) -> None:\n",
    "        self.c_proj.update()\n",
    "        self.c_attn.update()\n",
    "        return\n",
    "\n",
    "    def get_params(self) -> dict:\n",
    "        return {\n",
    "            \"c_attn\": [\n",
    "                compress_numpy_array(self.c_attn.weight),\n",
    "                compress_numpy_array(self.c_attn.bias),\n",
    "            ],\n",
    "            \"c_proj\": [\n",
    "                compress_numpy_array(self.c_proj.weight),\n",
    "                compress_numpy_array(self.c_proj.bias),\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    def load_params(self, state_dict: dict) -> None:\n",
    "        self.c_attn.weight = decompress_numpy_array(state_dict[\"c_attn\"][0])\n",
    "        self.c_attn.bias = decompress_numpy_array(state_dict[\"c_attn\"][1])\n",
    "        self.c_proj.weight = decompress_numpy_array(state_dict[\"c_proj\"][0])\n",
    "        self.c_proj.bias = decompress_numpy_array(state_dict[\"c_proj\"][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        z = self.c_attn(x)\n",
    "        q, k ,v  = z.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        \n",
    "        att = (q @ k.transpose(-2, -1)) \n",
    "        att = att *(1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        \n",
    "        # out = y\n",
    "        # t_fakeloss = (out * t_fake_upstr).sum()\n",
    "        # t_grad =  torch.autograd.grad(inputs=[x],outputs=[t_fakeloss])[0] \n",
    "        t_grad = None\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class TBlock(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act     = NewGELU(),\n",
    "            dropout = nn.Dropout(config.resid_pdrop),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.ln_1(x)\n",
    "        y2 = self.attn(y1)\n",
    "        y3 = x + y2\n",
    "        y4 = self.ln_2(y3)\n",
    "        y5 = self.mlpf(y4)\n",
    "        y6 = y3 + y5 # y6 is final grad\n",
    "\n",
    "        #fakeloss = (y6 * t_fake_upstr ).sum()\n",
    "        \n",
    "        #t_grad = torch.autograd.grad(outputs=[fakeloss],inputs=[x])[0]\n",
    "        #print(t_grad[(0,0)])\n",
    "        return y6# ,t_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 24])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "cp.random.seed(0)\n",
    "\n",
    "class Conf():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "config = Conf()\n",
    "config.n_embd = C\n",
    "config.resid_pdrop = 0.\n",
    "config.attn_pdrop = 0.\n",
    "config.n_head = 4 # reduced dim is 6, because 6 x 4 = 24\n",
    "config.block_size = T # context length\n",
    "t_block = TBlock(config)\n",
    "inp = np.random.random((B,T,C))\n",
    "\n",
    "t_inp = torch.tensor(inp,device=\"cpu\",dtype=torch.float32,requires_grad=True)\n",
    "# t_out ,t_grad= t_block.forward(t_inp)\n",
    "t_out = t_block.forward(t_inp)\n",
    "\n",
    "t_out.shape\n",
    "\n",
    "fakeloss = (t_out * t_fake_upstr ).sum()\n",
    "t_grad = torch.autograd.grad(outputs=[fakeloss],inputs=[t_inp])[0]\n",
    "print(t_grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block\n",
    "class Block:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        context_size: int,\n",
    "        n_heads: int,\n",
    "        batch_size: int,\n",
    "        lr: float,\n",
    "        dropout: float,\n",
    "    ) -> None:\n",
    "        def weight_init_func(size):\n",
    "            return cp.random.normal(size=size, loc=0.0, scale=0.02).astype(cp.float64)\n",
    "\n",
    "        def c_proj_init_func(size):\n",
    "            return cp.random.normal(\n",
    "                size=size, loc=0.0, scale=0.02 / math.sqrt(2 * 6) # 6 is n_layer default\n",
    "            ).astype(cp.float64)\n",
    "\n",
    "        def bias_init_func(size):\n",
    "            return cp.zeros(shape=size, dtype=cp.float64)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.context_size = context_size\n",
    "        self.n_heads = n_heads\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.ln_1 = LayerNorm(d_model, weight_init_func=weight_init_func)\n",
    "\n",
    "        self.attn = MultiHeadAttention(\n",
    "            d_model,\n",
    "            context_size,\n",
    "            n_heads,\n",
    "            batch_size,\n",
    "            lr,\n",
    "            dropout,\n",
    "            c_attn_weight_init_func=weight_init_func,\n",
    "            c_proj_weight_init_func=c_proj_init_func,\n",
    "            bias_init_func=bias_init_func,\n",
    "        )\n",
    "\n",
    "        self.ln_2 = LayerNorm(d_model, weight_init_func=weight_init_func)\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            d_model,\n",
    "            batch_size,\n",
    "            lr,\n",
    "            dropout,\n",
    "            c_fc_init_func=weight_init_func,\n",
    "            c_proj_init_func=c_proj_init_func,\n",
    "            bias_init_func=bias_init_func,\n",
    "        )\n",
    "    def forward(self, input: ArrayLike,train) -> cp.ndarray:\n",
    "\n",
    "        input = cp.asanyarray(input) # 0.99 # same\n",
    "        res = copy.deepcopy(input)\n",
    "        x = self.ln_1.forward(input) # 2.164 # same\n",
    "        x = self.attn.forward(x,train)[0] # 1.911 # not same: 2.16\n",
    "        x = res + x # 2.023\n",
    "        residual = copy.deepcopy(x)\n",
    "        x = self.ln_2.forward(x) # 2.798 # vs 3.092 # vs 2.46\n",
    "        x = self.mlp.forward(x,train) # 0.612\n",
    "        x = residual + x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def backward(self, upstream_grad):\n",
    "        \"\"\"\n",
    "        Computes gradients for the transformer block.\n",
    "\n",
    "        Args:\n",
    "            upstream_grad: Gradient from the subsequent layer.\n",
    "\n",
    "        Returns:\n",
    "            Gradient with respect to the input `x`.\n",
    "        \"\"\"\n",
    "        x = upstream_grad\n",
    "        # Backward pass for Residual Connection 2\n",
    "        x = self.mlp.backward(upstream_grad)\n",
    "        x = self.ln_2.backward(x)\n",
    "        x+=upstream_grad\n",
    "        # is correct\n",
    "        y = self.attn.backward(x)\n",
    "        y = self.ln_1.backward(y)\n",
    "        x = x + y\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update(self) -> None:\n",
    "        self.ln_2.update()\n",
    "        self.ln_1.update()\n",
    "        self.mlp.update()\n",
    "        self.attn.update()\n",
    "        return\n",
    "        # raise NotImplementedError(\"Implement the Block update\")\n",
    "\n",
    "    def state_dict(self) -> dict:\n",
    "        return {\n",
    "            \"ln_1\": [\n",
    "                compress_numpy_array(self.ln_1.weight),\n",
    "                compress_numpy_array(self.ln_1.bias),\n",
    "            ],\n",
    "            \"ln_2\": [\n",
    "                compress_numpy_array(self.ln_2.weight),\n",
    "                compress_numpy_array(self.ln_2.bias),\n",
    "            ],\n",
    "            \"mlp\": self.mlp.get_params(),\n",
    "            \"attn\": self.attn.get_params(),\n",
    "        }\n",
    "\n",
    "    def load_params(self, state_dict: dict) -> None:\n",
    "        self.ln_1.weight = decompress_numpy_array(state_dict[\"ln_1\"][0])\n",
    "        self.ln_1.bias = decompress_numpy_array(state_dict[\"ln_1\"][1])\n",
    "        self.ln_2.weight = decompress_numpy_array(state_dict[\"ln_2\"][0])\n",
    "        self.ln_2.bias = decompress_numpy_array(state_dict[\"ln_2\"][1])\n",
    "\n",
    "        self.mlp.load_params(state_dict[\"mlp\"])\n",
    "        self.attn.load_params(state_dict[\"attn\"])\n",
    "\n",
    "# MLP: \n",
    "    # def load_params(self, state_dict: dict) -> None:\n",
    "    #     self.c_fc.weight = decompress_numpy_array(state_dict[\"c_fc\"][0])\n",
    "    #     self.c_fc.bias = decompress_numpy_array(state_dict[\"c_fc\"][1])\n",
    "    #     self.c_proj.weight = decompress_numpy_array(state_dict[\"c_proj\"][0])\n",
    "    #     self.c_proj.bias = decompress_numpy_array(state_dict[\"c_proj\"][1])\n",
    "\n",
    "# attn:\n",
    "# def load_params(self, state_dict: dict) -> None:\n",
    "#         self.c_attn.weight = decompress_numpy_array(state_dict[\"c_attn\"][0])\n",
    "#         self.c_attn.bias = decompress_numpy_array(state_dict[\"c_attn\"][1])\n",
    "#         self.c_proj.weight = decompress_numpy_array(state_dict[\"c_proj\"][0])\n",
    "#         self.c_proj.bias = decompress_numpy_array(state_dict[\"c_proj\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[-0.25807333  0.54939985  0.9438534   0.8831857   0.44403353  0.76653683\n",
      "  1.0767456   0.93072057  0.6664855   0.00323181  0.82971454  0.06231073\n",
      "  0.2319231   0.94379604  0.25367013 -0.03447413  0.365102    1.5262859\n",
      "  0.6247262   0.801388    1.353657    1.6885219   0.4616583   1.2506313 ]\n",
      "[-0.25807319  0.54939985  0.94385338  0.88318546  0.44403343  0.76653687\n",
      "  1.07674568  0.93072042  0.66648558  0.00323188  0.8297146   0.06231068\n",
      "  0.23192303  0.94379614  0.25367012 -0.03447396  0.36510199  1.52628568\n",
      "  0.62472611  0.80138799  1.35365719  1.6885217   0.46165839  1.25063129]\n"
     ]
    }
   ],
   "source": [
    "c_Block = Block(d_model=C,context_size=T,n_heads=config.n_head,batch_size=B,lr=0.1,dropout=config.resid_pdrop,)\n",
    "# extract params from torch\n",
    "params = {\"ln_1\":[],\"ln_2\":[],\"mlp\":[],\"attn\":[]}\n",
    "params[\"ln_1\"].append(compress_numpy_array(cp.array( t_block.ln_1.weight.detach().numpy())))\n",
    "params[\"ln_1\"].append(compress_numpy_array(cp.array( t_block.ln_1.bias.detach().numpy()))) # bias will be 0 (24,)\n",
    "params[\"ln_2\"].append(compress_numpy_array(cp.array( t_block.ln_2.weight.detach().numpy())))\n",
    "params[\"ln_2\"].append(compress_numpy_array(cp.array( t_block.ln_2.bias.detach().numpy())))\n",
    "params[\"mlp\"]={\"c_fc\":[],\"c_proj\":[]}\n",
    "params[\"attn\"]={\"c_attn\":[],\"c_proj\":[]}\n",
    "params[\"mlp\"][\"c_fc\"].append(compress_numpy_array(cp.array(t_block.mlp.c_fc.weight.detach().numpy().T)))\n",
    "params[\"mlp\"][\"c_fc\"].append(compress_numpy_array(cp.array(t_block.mlp.c_fc.bias.detach().numpy())))\n",
    "params[\"mlp\"][\"c_proj\"].append(compress_numpy_array(cp.array(t_block.mlp.c_proj.weight.detach().numpy().T)))\n",
    "params[\"mlp\"][\"c_proj\"].append(compress_numpy_array(cp.array(t_block.mlp.c_proj.bias.detach().numpy())))\n",
    "params[\"attn\"][\"c_attn\"].append(compress_numpy_array(cp.array(t_block.attn.c_attn.weight.detach().numpy().T) ))\n",
    "params[\"attn\"][\"c_attn\"].append(compress_numpy_array(cp.array(t_block.attn.c_attn.bias.detach().numpy())))\n",
    "params[\"attn\"][\"c_proj\"].append(compress_numpy_array(cp.array(t_block.attn.c_proj.weight.detach().numpy().T)))\n",
    "params[\"attn\"][\"c_proj\"].append(compress_numpy_array(cp.array(t_block.attn.c_proj.bias.detach().numpy())))\n",
    "\n",
    "c_Block.load_params(params)\n",
    "\n",
    "c_inp = cp.array(inp)\n",
    "c_forward = c_Block.forward(c_inp,train=True)\n",
    "c_fake_upst = cp.array(fake_upst)\n",
    "c_grad = c_Block.backward(c_fake_upst)\n",
    "\n",
    "\n",
    "print()\n",
    "print(t_out.detach().numpy()[0][0])\n",
    "print(c_forward[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.3603)\n",
      "18.3643249226239\n",
      "tensor([ -0.5297,  15.4927,  -4.8079,  -8.4415,  14.7030,   1.4559,  11.8544,\n",
      "          7.6653,  -9.3051,  -5.8100,  14.6671,   8.9370,  -4.6227,  -2.8054,\n",
      "          4.5430,  -9.4771,   2.6450,   0.1734, -14.1352,  -3.7626, -11.3567,\n",
      "         -6.2144, -18.3086,   0.4401])\n",
      "[ -0.53784775  15.48721386  -4.81829949  -8.4723755   14.74274608\n",
      "   1.47139379  11.84151806   7.69470273  -9.27653668  -5.78913732\n",
      "  14.70168211   8.89696564  -4.62626788  -2.80444168   4.55083333\n",
      "  -9.46444707   2.63951683   0.18077429 -14.13560267  -3.72385471\n",
      " -11.38018738  -6.26906922 -18.3048754    0.39559602]\n"
     ]
    }
   ],
   "source": [
    "print(t_grad.max())\n",
    "print(c_grad.max())\n",
    "print(t_grad[(0,0)])\n",
    "print(c_grad[(0,0)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compare attn\n",
    "# t_attn = t_block.attn\n",
    "# c_attn = c_Block.attn\n",
    "\n",
    "# t_out ,t_grad= t_attn.forward(t_inp)\n",
    "# c_out,attn = c_attn.forward(c_inp,True)\n",
    "\n",
    "\n",
    "# c_grad = c_attn.backward(c_fake_upst)\n",
    "# print(t_out.detach().numpy()[(0,0)])\n",
    "# print(c_out[(0,0)])\n",
    "# # forward is the same!!\n",
    "# print(\"Now gradients:\")\n",
    "# print(t_grad.detach().numpy().shape,c_grad.shape,t_grad.max().item(),c_grad.max())\n",
    "# point = (0,0)\n",
    "# print(t_grad.detach().numpy()[point])\n",
    "# print(c_grad[point])\n",
    "# print((t_grad.detach().numpy()-cp.asnumpy(c_grad)).max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
