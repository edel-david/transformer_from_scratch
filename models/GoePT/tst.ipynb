{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import ArrayLike\n",
    "import cupy as cp\n",
    "import torch\n",
    "from typing import Union, Optional,Callable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper:\n",
    "def _multi_dim_matmul(\n",
    "        mat_a: cp.ndarray,\n",
    "        mat_b: cp.ndarray,\n",
    "        transpose_a: bool = False,\n",
    "        transpose_b: bool = False,\n",
    "        reshape_output: bool = True,\n",
    "    ) -> cp.ndarray:\n",
    "        \"\"\"\n",
    "        Replicate torch behavior of flattening all but the\n",
    "        last dimension of an input of the matrix multiplication\n",
    "        in linear layers. We implement this for both the first\n",
    "        and the last matrix in the matrix multiplication to\n",
    "        provide a unified operation for both the forward and\n",
    "        the backward pass.\n",
    "        \"\"\"\n",
    "\n",
    "        if (len(mat_a.shape) > 2) or (len(mat_b.shape) > 2):\n",
    "            # Dimension handling.\n",
    "            # We should refactor this if we find the time.\n",
    "\n",
    "            dims_internal_mat_a = (\n",
    "                mat_a.shape\n",
    "                if len(mat_a.shape) <= 2\n",
    "                else (cp.prod(cp.array(mat_a.shape[:-1])).item(), mat_a.shape[-1])\n",
    "            )\n",
    "\n",
    "            dims_internal_mat_b = (\n",
    "                mat_b.shape\n",
    "                if len(mat_b.shape) <= 2\n",
    "                else (cp.prod(cp.array(mat_b.shape[:-1])).item(), mat_b.shape[-1])\n",
    "            )\n",
    "\n",
    "            mat_a_shape = mat_a.shape[::-1] if transpose_a else mat_a.shape\n",
    "            mat_b_shape = mat_b.shape[::-1] if transpose_b else mat_b.shape\n",
    "\n",
    "            dims_out_first = (\n",
    "                mat_a.shape[:-1]\n",
    "                if reshape_output\n",
    "                else (\n",
    "                    dims_internal_mat_a[1] if transpose_a else dims_internal_mat_a[0],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            dims_out = (*dims_out_first, mat_b_shape[-1])\n",
    "\n",
    "            def mat_a_transform():\n",
    "                if transpose_a:\n",
    "                    return mat_a.reshape(dims_internal_mat_a).T\n",
    "                else:\n",
    "                    return mat_a.reshape(dims_internal_mat_a)\n",
    "\n",
    "            def mat_b_transform():\n",
    "                if transpose_b:\n",
    "                    return mat_b.reshape(dims_internal_mat_b).T\n",
    "                else:\n",
    "                    return mat_b.reshape(dims_internal_mat_b)\n",
    "\n",
    "            return cp.matmul(mat_a_transform(), mat_b_transform()).reshape(dims_out)\n",
    "\n",
    "        else:\n",
    "            return cp.matmul(mat_a, mat_b.T) if transpose_b else cp.matmul(mat_a, mat_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing GELU backwards pass by comparing output gradient to torch GELU gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU:\n",
    "    def __init__(self) -> None:\n",
    "        self._sqrt_of_2_by_pi = cp.sqrt(2 / cp.pi)\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, input: ArrayLike) -> cp.ndarray:\n",
    "        self.input = cp.asanyarray(input)\n",
    "        return (\n",
    "            0.5\n",
    "            * input\n",
    "            * (\n",
    "                1\n",
    "                + cp.tanh(\n",
    "                    self._sqrt_of_2_by_pi * (input + 0.044715 * cp.power(input, 3))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def backward(self, grad_output: ArrayLike) -> cp.ndarray:\n",
    "        # raise NotImplementedError(\"Implement the GELU backward path\")\n",
    "        x = self.input\n",
    "        m1 = self._sqrt_of_2_by_pi\n",
    "        m2 = 0.044715\n",
    "        m3 = m1 * (x+m2 * x**3)\n",
    "        tanhm3 = cp.tanh(\n",
    "                    m3\n",
    "                )\n",
    "        first = 0.5 * (\n",
    "                1\n",
    "                + tanhm3\n",
    "            )\n",
    "        second = x/2 * (1- tanhm3**2) * (m1+2*x**2 * m2*m1)\n",
    "        grad_out = (first + second) * grad_output\n",
    "        return grad_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Gelu= GELU()\n",
    "a=cp.random.random((5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_torch = torch.tensor(a.copy(),requires_grad=True,device=\"cpu\")\n",
    "b_t=torch.nn.functional.gelu(a_torch)\n",
    "\n",
    "b_n = Gelu.forward(a.copy())\n",
    "b_n_grad = Gelu.backward(cp.ones((5,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34228479 0.79604536 0.18636156 0.05129523 0.18866275]\n",
      "[0.34226806 0.79590748 0.18635901 0.0512952  0.1886601 ]\n"
     ]
    }
   ],
   "source": [
    "print(b_t.clone().cpu().detach().numpy())\n",
    "print(b_n) \n",
    "# close enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86305331 1.0633282  0.73281609 0.57582989 0.73507947]\n",
      "[0.8650387  1.07250098 0.7333096  0.5758464  0.73558789]\n",
      "[-1.98539357e-03 -9.17278582e-03 -4.93511413e-04 -1.65041191e-05\n",
      " -5.08424395e-04]\n"
     ]
    }
   ],
   "source": [
    "print(b_n_grad)\n",
    "torch_sum = b_t.sum() # the gradient of sum equals the cp.ones((5,))\n",
    "grads = cp.array(torch.autograd.grad(outputs=[torch_sum],inputs=[a_torch])[0].numpy())\n",
    "print(grads)\n",
    "\n",
    "diff = (b_n_grad- grads)\n",
    "print(diff)\n",
    "# diff is very smol\n",
    "assert(abs(diff).max() <= 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comparing LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(\n",
    "        self,\n",
    "        normalized_shape: Union[int, tuple[int]],\n",
    "        eps: float = 1e-05,\n",
    "        lr: float = 1e-3,\n",
    "        weight_init =None,\n",
    "        bias_init=None,\n",
    "    ) -> None:\n",
    "\n",
    "        self.normalized_shape = (\n",
    "            (normalized_shape,)\n",
    "            if isinstance(normalized_shape, int)\n",
    "            else normalized_shape\n",
    "        )\n",
    "\n",
    "        self.eps = eps\n",
    "        self.lr = lr\n",
    "        self.weight =weight_init\n",
    "\n",
    "        self.bias = bias_init\n",
    "\n",
    "        self.axis = None\n",
    "\n",
    "        self.input = None\n",
    "\n",
    "        self.grad_weight = None\n",
    "        self.grad_bias = None\n",
    "\n",
    "        self.x_centered = None\n",
    "        self.stddev_inv = None\n",
    "\n",
    "    def forward(self, input: ArrayLike) -> cp.ndarray:\n",
    "\n",
    "        input = cp.asanyarray(input)\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        self.axis = tuple(range(-len(self.normalized_shape), 0))\n",
    "        #  -n,..., -2 , -1 ohne 0\n",
    "\n",
    "        mean = cp.mean(input, axis=self.axis, keepdims=True)\n",
    "        self.x_centered = input - mean\n",
    "        var = cp.var(\n",
    "            input, axis=self.axis, keepdims=True, # mean=mean\n",
    "        )  # can we pass the mean to the var()?  YES (with newer numpy versions)!\n",
    "        # the var stays the same after centering. Usefull for gradient calculation (not really)\n",
    "        self.stddev_inv = 1 / cp.sqrt(var + self.eps)\n",
    "\n",
    "        output = self.x_centered * self.stddev_inv\n",
    "\n",
    "        return self.weight * output + self.bias\n",
    "\n",
    "    def _rip_backward(self,grad):\n",
    "        grad_main = grad * self.weight # no transpose needed because multiplication is element wise\n",
    "        outer = cp.einsum(\"...i,...j->...ij\", self.x_centered, self.x_centered) \n",
    "        reshaped_invvar = self.stddev_inv.reshape((*self.stddev_inv.shape,1))\n",
    "        part = outer * reshaped_invvar **3 / C\n",
    "        part2 = (reshaped_invvar -part)\n",
    "        grad_centered =  cp.matmul(grad_main.reshape((B,T,1,C)),part2)\n",
    "        grad_mean_input = cp.full((B,T,C,C), -1 / self.input.shape[-1])\n",
    "        grad_mean_input+= cp.diag(cp.ones((self.input.shape[-1],))).reshape((1,1,C,C))\n",
    "        grad_input = cp.matmul(grad_centered,grad_mean_input)\n",
    "\n",
    "        grad_out = grad_input\n",
    "        return grad_out.squeeze() # does not work\n",
    "\n",
    "    def backward(self, grad: ArrayLike) -> cp.ndarray:\n",
    "        B,T,C = self.input.shape\n",
    "        self.grad_bias = grad.mean(axis=(0, 1))  # upstream gradient * 1.\n",
    "        self.grad_weight = (\n",
    "            grad * (self.x_centered * self.stddev_inv)\n",
    "        ).mean(axis=(0, 1))  # upstream * centered * invvar\n",
    "\n",
    "        normalized = self.x_centered * self.stddev_inv\n",
    "        grad_normalized = grad * self.weight\n",
    "        grad_x = grad_normalized - grad_normalized.mean(-1,keepdims=True) - normalized * (grad_normalized *normalized).mean(-1,keepdims=True)\n",
    "        grad_x = grad_x * self.stddev_inv\n",
    "        return grad_x\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        self.weight -= self.lr * self.grad_weight\n",
    "        self.bias -= self.lr * self.grad_bias\n",
    "        return\n",
    "        # raise NotImplementedError(\"Implement the LayerNorm update routine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4) 0.6759062075736666\n"
     ]
    }
   ],
   "source": [
    "B,T,C = (2,3,4)\n",
    "\n",
    "i_np = np.random.random((B,T,C)) # input_array\n",
    "#i_np = np.array([1,2,3]).reshape((B,T,C))\n",
    "print(i_np.shape,i_np.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm((4,), eps=1e-05, elementwise_affine=True) torch.float64\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5191, -4.7207, -0.1122,  5.3520],\n",
       "         [ 0.2664,  1.6462, -0.4588, -1.4537],\n",
       "         [-2.9517, -0.6816,  1.6801,  1.9532]],\n",
       "\n",
       "        [[ 1.2894, -1.5235, -1.2015,  1.4356],\n",
       "         [-0.4970, -1.4635,  9.6867, -7.7262],\n",
       "         [ 1.3423,  1.7049, -4.1252,  1.0780]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_norm = torch.nn.LayerNorm((C,) ,device=\"cpu\",dtype=torch.float64)\n",
    "print(t_norm,t_norm.weight.dtype)\n",
    "print(t_norm.weight.shape)\n",
    "print(t_norm.bias.shape)\n",
    "i_t = torch.tensor(i_np,device=\"cpu\",requires_grad=True,dtype=t_norm.weight.dtype)\n",
    "t_out = t_norm.forward(i_t)\n",
    "t_dout = torch.randn(B,T,C)\n",
    "t_loss = (t_out * t_dout).sum()  # the gradient of sum equals the cp.ones((5,))\n",
    "#t_grads = cp.array(torch.autograd.grad(outputs=[torch_sum],inputs=[i_t])[0].numpy())\n",
    "#print(t_grads.shape,t_grads.mean())\n",
    "#print(t_grads.squeeze())\n",
    "t_loss.backward()\n",
    "i_t.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4) -8.326672684688674e-16\n",
      "[[[-0.51912519 -4.72066233 -0.11216766  5.35195518]\n",
      "  [ 0.2664128   1.64616202 -0.458826   -1.45374883]\n",
      "  [-2.95174785 -0.68156979  1.68008565  1.95323199]]\n",
      "\n",
      " [[ 1.28937416 -1.52346011 -1.20148196  1.43556791]\n",
      "  [-0.49700066 -1.46347688  9.68665205 -7.72617452]\n",
      "  [ 1.34228999  1.70490914 -4.12523728  1.07803815]]]\n"
     ]
    }
   ],
   "source": [
    "s_norm = LayerNorm((C,),weight_init=cp.array(t_norm.weight.detach().numpy()),bias_init=cp.array(t_norm.bias.detach().numpy()))\n",
    "c_np = cp.array(i_np,dtype=cp.float64)\n",
    "c_result = s_norm.forward(c_np)\n",
    "upstr = cp.ones((B,T,C))\n",
    "c_grads = s_norm.backward(cp.array(t_dout.detach().numpy()))\n",
    "print(c_grads.shape,c_grads.mean())\n",
    "print(c_grads.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5646, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "3.3306690738754696e-15\n"
     ]
    }
   ],
   "source": [
    "print(torch_sum)\n",
    "print(c_result.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# inspect forward:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mt_result\u001b[49m\u001b[38;5;241m.\u001b[39mmax(),c_result\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# forward is the same!\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(t_result\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mshape,t_result\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m))) \u001b[38;5;66;03m# the average over the last dim is indeed 0\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 't_result' is not defined"
     ]
    }
   ],
   "source": [
    "# inspect forward:\n",
    "print(t_result.max(),c_result.max())\n",
    "# forward is the same!\n",
    "print(t_result.mean(dim=(2)).shape,t_result.mean(dim=(2))) # the average over the last dim is indeed 0\n",
    "print(t_result.var(dim=(2)).shape,t_result.var(dim=(2))) # and var is 1\n",
    "# now c\n",
    "print(c_result.mean(axis=(2)).shape,t_result.mean(axis=(2))) # the average over the last dim is indeed 0\n",
    "print(c_result.var(axis=(2)).shape,t_result.var(axis=(2)))\n",
    "# perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.220446049250313e-16 -2.2109675761565468e-31\n",
      "2.3830173178551396e-30 1.3069015216202024e-30\n",
      "3.552713678800501e-15 3.0933114078038186e-15\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2, 2, 3) (2, 3, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[295], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(t_grads\u001b[38;5;241m.\u001b[39mvar(),c_grads\u001b[38;5;241m.\u001b[39mvar())\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(t_grads\u001b[38;5;241m.\u001b[39mmax(),c_grads\u001b[38;5;241m.\u001b[39mmax())\n\u001b[0;32m----> 6\u001b[0m diff \u001b[38;5;241m=\u001b[39m \u001b[43mt_grads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc_grads\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(diff\u001b[38;5;241m.\u001b[39mmean()) \n\u001b[1;32m      8\u001b[0m point \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32mcupy/_core/core.pyx:1279\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__sub__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/_kernel.pyx:1315\u001b[0m, in \u001b[0;36mcupy._core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/internal.pyx:381\u001b[0m, in \u001b[0;36mcupy._core.internal._broadcast_core\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2, 2, 3) (2, 3, 4)"
     ]
    }
   ],
   "source": [
    "# backward:\n",
    "c_grads=c_grads.squeeze()\n",
    "print(t_grads.mean(),c_grads.mean())\n",
    "print(t_grads.var(),c_grads.var())\n",
    "print(t_grads.max(),c_grads.max())\n",
    "diff = t_grads - c_grads\n",
    "print(diff.mean()) \n",
    "point = (0,0,0)\n",
    "# print(t_grads[point])\n",
    "# print(c_grads[point])\n",
    "# print(t_grads[point]-c_grads[point])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 2)\n",
      "[[[1 2]\n",
      "  [2 3]\n",
      "  [3 4]]\n",
      "\n",
      " [[4 5]\n",
      "  [5 6]\n",
      "  [6 7]]]\n",
      "(2, 3, 2, 2)\n",
      "[[[[ 1  2]\n",
      "   [ 2  4]]\n",
      "\n",
      "  [[ 4  6]\n",
      "   [ 6  9]]\n",
      "\n",
      "  [[ 9 12]\n",
      "   [12 16]]]\n",
      "\n",
      "\n",
      " [[[16 20]\n",
      "   [20 25]]\n",
      "\n",
      "  [[25 30]\n",
      "   [30 36]]\n",
      "\n",
      "  [[36 42]\n",
      "   [42 49]]]]\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# tst\n",
    "init = np.array([[[1, 2], [2, 3], [3, 4]],\n",
    "            [[4, 5], [5, 6], [6, 7]]])  # Shape (2, 3,2)\n",
    "print(init.shape)\n",
    "print(init)\n",
    "result = np.einsum(\"...i,...j->...ij\", init, init)\n",
    "print(result.shape)\n",
    "print(result) # is the same as result.transpose(0,1,3,2) due to the symerty in the last dimension\n",
    "a=0\n",
    "b=1\n",
    "print(result[(a,b,0,1)])  # == result[a,b,0] * result[a,b,1]\n",
    "print( 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2, 3, 3)\n",
      "(4, 2, 3, 1)\n",
      "(4, 2, 3, 1)\n",
      "[[4610]\n",
      " [4826]\n",
      " [5042]]\n",
      "4610\n"
     ]
    }
   ],
   "source": [
    "dima = (4,2,3,3)\n",
    "A = np.arange(18*4).reshape(dima)\n",
    "B = np.arange(2,26).reshape((4,2,3,1))\n",
    "print(A.shape)\n",
    "print(B.shape)\n",
    "res= np.matmul(A,B)\n",
    "print(res.shape)\n",
    "print(res[(3,1)])\n",
    "manual = A[(3,1,0,0)] * B[(3,1,0,0)] + A[(3,1,0,1)] * B[(3,1,1,0)] +A[(3,1,0,2)] * B[(3,1,2,0)]\n",
    "print(manual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
