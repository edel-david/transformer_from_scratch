{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import ArrayLike\n",
    "import cupy as cp\n",
    "import torch\n",
    "from typing import Union, Optional,Callable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing GELU backwards pass by comparing output gradient to torch GELU gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU:\n",
    "    def __init__(self) -> None:\n",
    "        self._sqrt_of_2_by_pi = cp.sqrt(2 / cp.pi)\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, input: ArrayLike) -> cp.ndarray:\n",
    "        self.input = cp.asanyarray(input)\n",
    "        return (\n",
    "            0.5\n",
    "            * input\n",
    "            * (\n",
    "                1\n",
    "                + cp.tanh(\n",
    "                    self._sqrt_of_2_by_pi * (input + 0.044715 * cp.power(input, 3))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def backward(self, grad_output: ArrayLike) -> cp.ndarray:\n",
    "        # raise NotImplementedError(\"Implement the GELU backward path\")\n",
    "        x = self.input\n",
    "        m1 = self._sqrt_of_2_by_pi\n",
    "        m2 = 0.044715\n",
    "        m3 = m1 * (x+m2 * x**3)\n",
    "        tanhm3 = cp.tanh(\n",
    "                    m3\n",
    "                )\n",
    "        first = 0.5 * (\n",
    "                1\n",
    "                + tanhm3\n",
    "            )\n",
    "        second = x/2 * (1- tanhm3**2) * (m1+2*x**2 * m2*m1)\n",
    "        grad_out = (first + second) * grad_output\n",
    "        return grad_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Gelu= GELU()\n",
    "a=cp.random.random((5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_torch = torch.tensor(a.copy(),requires_grad=True,device=\"cpu\")\n",
    "b_t=torch.nn.functional.gelu(a_torch)\n",
    "\n",
    "b_n = Gelu.forward(a.copy())\n",
    "b_n_grad = Gelu.backward(cp.ones((5,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.53843265 0.78671516 0.40918719 0.77219431 0.62633567]\n",
      "[0.53837527 0.78658037 0.40915942 0.77206434 0.62625296]\n"
     ]
    }
   ],
   "source": [
    "print(b_t.clone().cpu().detach().numpy())\n",
    "print(b_n) \n",
    "# close enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97547696 1.06107309 0.90687149 1.05744095 1.01192964]\n",
      "[0.98035172 1.07009616 0.90974865 1.06622904 1.0182753 ]\n",
      "[-0.00487475 -0.00902307 -0.00287715 -0.00878809 -0.00634566]\n"
     ]
    }
   ],
   "source": [
    "print(b_n_grad)\n",
    "torch_sum = b_t.sum() # the gradient of sum equals the cp.ones((5,))\n",
    "grads = cp.array(torch.autograd.grad(outputs=[torch_sum],inputs=[a_torch])[0].numpy())\n",
    "print(grads)\n",
    "\n",
    "diff = (b_n_grad- grads)\n",
    "print(diff)\n",
    "# diff is very smol\n",
    "assert(abs(diff).max() <= 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comparing LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(\n",
    "        self,\n",
    "        normalized_shape: Union[int, tuple[int]],\n",
    "        eps: float = 1e-05,\n",
    "        lr: float = 1e-3,\n",
    "        weight_init =None,\n",
    "        bias_init=None,\n",
    "    ) -> None:\n",
    "\n",
    "        self.normalized_shape = (\n",
    "            (normalized_shape,)\n",
    "            if isinstance(normalized_shape, int)\n",
    "            else normalized_shape\n",
    "        )\n",
    "\n",
    "        self.eps = eps\n",
    "        self.lr = lr\n",
    "        self.weight =weight_init\n",
    "\n",
    "        self.bias = bias_init\n",
    "\n",
    "        self.axis = None\n",
    "\n",
    "        self.input = None\n",
    "\n",
    "        self.grad_weight = None\n",
    "        self.grad_bias = None\n",
    "\n",
    "        self.x_centered = None\n",
    "        self.stddev_inv = None\n",
    "\n",
    "    def forward(self, input: ArrayLike) -> cp.ndarray:\n",
    "\n",
    "        input = cp.asanyarray(input)\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        self.axis = tuple(range(-len(self.normalized_shape), 0))\n",
    "        #  -n,..., -2 , -1 ohne 0\n",
    "\n",
    "        mean = cp.mean(input, axis=self.axis, keepdims=True)\n",
    "        var = cp.var(\n",
    "            input, axis=self.axis, keepdims=True, # mean=mean\n",
    "        )  # can we pass the mean to the var()?  YES (with newer numpy versions)!\n",
    "        # the var stays the same after centering. Usefull for gradient calculation (not really)\n",
    "        self.x_centered = input - mean\n",
    "        self.stddev_inv = 1 / cp.sqrt(var + self.eps)\n",
    "\n",
    "        output = self.x_centered * self.stddev_inv\n",
    "\n",
    "        return self.weight * output + self.bias\n",
    "\n",
    "    def backward(self, grad: ArrayLike) -> cp.ndarray:\n",
    "        self.grad_bias = grad  # upstream gradient * 1.\n",
    "        self.grad_weight = (\n",
    "            grad * self.x_centered * self.stddev_inv\n",
    "        )  # upstream * centered * invvar\n",
    "\n",
    "        # fuck this is hard\n",
    "        grad = grad * self.weight.transpose()  # add dims to transpose\n",
    "        grad = grad * self.stddev_inv  # .squeeze()\n",
    "        # grad_out = grad.reshape((*grad.shape, 1)) * (\n",
    "        #     -2 * cp.power(self.x_centered, 2) * cp.power(self.stddev_inv, 2)\n",
    "        #     + (self.stddev_inv * (1 - self.normalized_shape[-1]))\n",
    "        # )  # TODO: check\n",
    "        #\n",
    "        grad_out = grad * (1 - 1 / self.input.shape[-1])\n",
    "\n",
    "        return grad_out\n",
    "\n",
    "    def update(self):\n",
    "        self.weight -= self.lr * self.grad_weight.mean(axis=(0, 1))\n",
    "        self.bias -= self.lr * self.grad_bias.mean(axis=(0, 1))\n",
    "        return\n",
    "        # raise NotImplementedError(\"Implement the LayerNorm update routine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 256, 384) 0.4995360355350611\n"
     ]
    }
   ],
   "source": [
    "B,T,C = (16,256,384)\n",
    "h = 6\n",
    "i_np = np.random.random((B,T,C)) # input_array\n",
    "print(i_np.shape,i_np.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm((384,), eps=1e-05, elementwise_affine=True) torch.float32\n",
      "torch.Size([384])\n",
      "torch.Size([384])\n",
      "(16, 256, 384) 3.0994415e-06\n"
     ]
    }
   ],
   "source": [
    "t_norm = torch.nn.LayerNorm((C,) ,device=\"cpu\")\n",
    "print(t_norm,t_norm.weight.dtype)\n",
    "print(t_norm.weight.shape)\n",
    "print(t_norm.bias.shape)\n",
    "i_t = torch.tensor(i_np,device=\"cpu\",requires_grad=True,dtype=t_norm.weight.dtype)\n",
    "result = t_norm.forward(i_t)\n",
    "torch_sum = result.sum() # the gradient of sum equals the cp.ones((5,))\n",
    "grads = cp.array(torch.autograd.grad(outputs=[torch_sum],inputs=[i_t])[0].numpy())\n",
    "print(grads.shape,grads.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_norm = LayerNorm((C,),weight_init=t_norm.weight,bias_init=t_norm.bias)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
