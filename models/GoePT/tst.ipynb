{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import ArrayLike\n",
    "import cupy as cp\n",
    "import torch\n",
    "from typing import Union, Optional,Callable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper:\n",
    "def _multi_dim_matmul(\n",
    "        mat_a: cp.ndarray,\n",
    "        mat_b: cp.ndarray,\n",
    "        transpose_a: bool = False,\n",
    "        transpose_b: bool = False,\n",
    "        reshape_output: bool = True,\n",
    "    ) -> cp.ndarray:\n",
    "        \"\"\"\n",
    "        Replicate torch behavior of flattening all but the\n",
    "        last dimension of an input of the matrix multiplication\n",
    "        in linear layers. We implement this for both the first\n",
    "        and the last matrix in the matrix multiplication to\n",
    "        provide a unified operation for both the forward and\n",
    "        the backward pass.\n",
    "        \"\"\"\n",
    "\n",
    "        if (len(mat_a.shape) > 2) or (len(mat_b.shape) > 2):\n",
    "            # Dimension handling.\n",
    "            # We should refactor this if we find the time.\n",
    "\n",
    "            dims_internal_mat_a = (\n",
    "                mat_a.shape\n",
    "                if len(mat_a.shape) <= 2\n",
    "                else (cp.prod(cp.array(mat_a.shape[:-1])).item(), mat_a.shape[-1])\n",
    "            )\n",
    "\n",
    "            dims_internal_mat_b = (\n",
    "                mat_b.shape\n",
    "                if len(mat_b.shape) <= 2\n",
    "                else (cp.prod(cp.array(mat_b.shape[:-1])).item(), mat_b.shape[-1])\n",
    "            )\n",
    "\n",
    "            mat_a_shape = mat_a.shape[::-1] if transpose_a else mat_a.shape\n",
    "            mat_b_shape = mat_b.shape[::-1] if transpose_b else mat_b.shape\n",
    "\n",
    "            dims_out_first = (\n",
    "                mat_a.shape[:-1]\n",
    "                if reshape_output\n",
    "                else (\n",
    "                    dims_internal_mat_a[1] if transpose_a else dims_internal_mat_a[0],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            dims_out = (*dims_out_first, mat_b_shape[-1])\n",
    "\n",
    "            def mat_a_transform():\n",
    "                if transpose_a:\n",
    "                    return mat_a.reshape(dims_internal_mat_a).T\n",
    "                else:\n",
    "                    return mat_a.reshape(dims_internal_mat_a)\n",
    "\n",
    "            def mat_b_transform():\n",
    "                if transpose_b:\n",
    "                    return mat_b.reshape(dims_internal_mat_b).T\n",
    "                else:\n",
    "                    return mat_b.reshape(dims_internal_mat_b)\n",
    "\n",
    "            return cp.matmul(mat_a_transform(), mat_b_transform()).reshape(dims_out)\n",
    "\n",
    "        else:\n",
    "            return cp.matmul(mat_a, mat_b.T) if transpose_b else cp.matmul(mat_a, mat_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing GELU backwards pass by comparing output gradient to torch GELU gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU:\n",
    "    def __init__(self) -> None:\n",
    "        self._sqrt_of_2_by_pi = cp.sqrt(2 / cp.pi)\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, input: ArrayLike) -> cp.ndarray:\n",
    "        self.input = cp.asanyarray(input)\n",
    "        return (\n",
    "            0.5\n",
    "            * input\n",
    "            * (\n",
    "                1\n",
    "                + cp.tanh(\n",
    "                    self._sqrt_of_2_by_pi * (input + 0.044715 * cp.power(input, 3))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def backward(self, grad_output: ArrayLike) -> cp.ndarray:\n",
    "        # raise NotImplementedError(\"Implement the GELU backward path\")\n",
    "        x = self.input\n",
    "        m1 = self._sqrt_of_2_by_pi\n",
    "        m2 = 0.044715\n",
    "        m3 = m1 * (x+m2 * x**3)\n",
    "        tanhm3 = cp.tanh(\n",
    "                    m3\n",
    "                )\n",
    "        first = 0.5 * (\n",
    "                1\n",
    "                + tanhm3\n",
    "            )\n",
    "        second = x/2 * (1- tanhm3**2) * (m1+2*x**2 * m2*m1)\n",
    "        grad_out = (first + second) * grad_output\n",
    "        return grad_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "CUDADriverError",
     "evalue": "CUDA_ERROR_ASSERT: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCUDADriverError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Gelu\u001b[38;5;241m=\u001b[39m \u001b[43mGELU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m a\u001b[38;5;241m=\u001b[39mcp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom((\u001b[38;5;241m5\u001b[39m,))\n",
      "Cell \u001b[0;32mIn[43], line 3\u001b[0m, in \u001b[0;36mGELU.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sqrt_of_2_by_pi \u001b[38;5;241m=\u001b[39m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32mcupy/_core/_kernel.pyx:1377\u001b[0m, in \u001b[0;36mcupy._core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/cuda/function.pyx:237\u001b[0m, in \u001b[0;36mcupy.cuda.function.Function.linear_launch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/cuda/function.pyx:205\u001b[0m, in \u001b[0;36mcupy.cuda.function._launch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy_backends/cuda/api/driver.pyx:273\u001b[0m, in \u001b[0;36mcupy_backends.cuda.api.driver.launchKernel\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy_backends/cuda/api/driver.pyx:63\u001b[0m, in \u001b[0;36mcupy_backends.cuda.api.driver.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCUDADriverError\u001b[0m: CUDA_ERROR_ASSERT: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "\n",
    "Gelu= GELU()\n",
    "a=cp.random.random((5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_torch = torch.tensor(a.copy(),requires_grad=True,device=\"cpu\")\n",
    "b_t=torch.nn.functional.gelu(a_torch)\n",
    "\n",
    "b_n = Gelu.forward(a.copy())\n",
    "b_n_grad = Gelu.backward(cp.ones((5,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79185314 0.53974328 0.05837285 0.4731661  0.67837019]\n",
      "[0.79171665 0.53968555 0.05837281 0.47312496 0.67827111]\n"
     ]
    }
   ],
   "source": [
    "print(b_t.clone().cpu().detach().numpy())\n",
    "print(b_n) \n",
    "# close enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.06232248 0.97607672 0.58544828 0.94326574 1.03015566]\n",
      "[1.07142812 0.98097299 0.58547192 0.94709618 1.03738257]\n",
      "[-9.10564398e-03 -4.89626751e-03 -2.36383523e-05 -3.83043649e-03\n",
      " -7.22690840e-03]\n"
     ]
    }
   ],
   "source": [
    "print(b_n_grad)\n",
    "torch_sum = b_t.sum() # the gradient of sum equals the cp.ones((5,))\n",
    "grads = cp.array(torch.autograd.grad(outputs=[torch_sum],inputs=[a_torch])[0].numpy())\n",
    "print(grads)\n",
    "\n",
    "diff = (b_n_grad- grads)\n",
    "print(diff)\n",
    "# diff is very smol\n",
    "assert(abs(diff).max() <= 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comparing LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(\n",
    "        self,\n",
    "        normalized_shape: Union[int, tuple[int]],\n",
    "        eps: float = 1e-05,\n",
    "        lr: float = 1e-3,\n",
    "        weight_init =None,\n",
    "        bias_init=None,\n",
    "    ) -> None:\n",
    "\n",
    "        self.normalized_shape = (\n",
    "            (normalized_shape,)\n",
    "            if isinstance(normalized_shape, int)\n",
    "            else normalized_shape\n",
    "        )\n",
    "\n",
    "        self.eps = eps\n",
    "        self.lr = lr\n",
    "        self.weight =weight_init\n",
    "\n",
    "        self.bias = bias_init\n",
    "\n",
    "        self.axis = None\n",
    "\n",
    "        self.input = None\n",
    "\n",
    "        self.grad_weight = None\n",
    "        self.grad_bias = None\n",
    "\n",
    "        self.x_centered = None\n",
    "        self.stddev_inv = None\n",
    "\n",
    "    def forward(self, input: ArrayLike) -> cp.ndarray:\n",
    "\n",
    "        input = cp.asanyarray(input)\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        self.axis = tuple(range(-len(self.normalized_shape), 0))\n",
    "        #  -n,..., -2 , -1 ohne 0\n",
    "\n",
    "        mean = cp.mean(input, axis=self.axis, keepdims=True)\n",
    "        self.x_centered = input - mean\n",
    "        var = cp.var(\n",
    "            input, axis=self.axis, keepdims=True, # mean=mean\n",
    "        )  # can we pass the mean to the var()?  YES (with newer numpy versions)!\n",
    "        # the var stays the same after centering. Usefull for gradient calculation (not really)\n",
    "        self.stddev_inv = 1 / cp.sqrt(var + self.eps)\n",
    "\n",
    "        output = self.x_centered * self.stddev_inv\n",
    "\n",
    "        return self.weight * output + self.bias\n",
    "\n",
    "    def _rip_backward(self,grad):\n",
    "        grad_main = grad * self.weight # no transpose needed because multiplication is element wise\n",
    "        outer = cp.einsum(\"...i,...j->...ij\", self.x_centered, self.x_centered) \n",
    "        reshaped_invvar = self.stddev_inv.reshape((*self.stddev_inv.shape,1))\n",
    "        part = outer * reshaped_invvar **3 / C\n",
    "        part2 = (reshaped_invvar -part)\n",
    "        grad_centered =  cp.matmul(grad_main.reshape((B,T,1,C)),part2)\n",
    "        grad_mean_input = cp.full((B,T,C,C), -1 / self.input.shape[-1])\n",
    "        grad_mean_input+= cp.diag(cp.ones((self.input.shape[-1],))).reshape((1,1,C,C))\n",
    "        grad_input = cp.matmul(grad_centered,grad_mean_input)\n",
    "\n",
    "        grad_out = grad_input\n",
    "        return grad_out.squeeze() # does not work\n",
    "\n",
    "    def backward(self, grad: ArrayLike) -> cp.ndarray:\n",
    "        B,T,C = self.input.shape\n",
    "        self.grad_bias = grad.mean(axis=(0, 1))  # upstream gradient * 1.\n",
    "        self.grad_weight = (\n",
    "            grad * (self.x_centered * self.stddev_inv)\n",
    "        ).mean(axis=(0, 1))  # upstream * centered * invvar\n",
    "\n",
    "        normalized = self.x_centered * self.stddev_inv\n",
    "        grad_normalized = grad * self.weight\n",
    "        grad_x = grad_normalized - grad_normalized.mean(-1,keepdims=True) - normalized * (grad_normalized *normalized).mean(-1,keepdims=True)\n",
    "        grad_x = grad_x * self.stddev_inv\n",
    "        return grad_x\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        self.weight -= self.lr * self.grad_weight\n",
    "        self.bias -= self.lr * self.grad_bias\n",
    "        return\n",
    "        # raise NotImplementedError(\"Implement the LayerNorm update routine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4) 0.5112360466896045\n"
     ]
    }
   ],
   "source": [
    "B,T,C = (2,3,4)\n",
    "\n",
    "i_np = np.random.random((B,T,C)) # input_array\n",
    "#i_np = np.array([1,2,3]).reshape((B,T,C))\n",
    "print(i_np.shape,i_np.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm((4,), eps=1e-05, elementwise_affine=True) torch.float64\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.9628, -0.3869,  0.0784, -2.6543],\n",
       "         [ 1.3553,  0.3200, -2.0612,  0.3859],\n",
       "         [ 7.0598,  4.3015, -6.2351, -5.1261]],\n",
       "\n",
       "        [[ 0.6526,  0.5360, -0.0959, -1.0926],\n",
       "         [ 3.6040,  3.0735,  0.1594, -6.8369],\n",
       "         [-4.9914,  4.2253,  3.9680, -3.2019]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_norm = torch.nn.LayerNorm((C,) ,device=\"cpu\",dtype=torch.float64)\n",
    "print(t_norm,t_norm.weight.dtype)\n",
    "print(t_norm.weight.shape)\n",
    "print(t_norm.bias.shape)\n",
    "i_t = torch.tensor(i_np,device=\"cpu\",requires_grad=True,dtype=t_norm.weight.dtype)\n",
    "t_out = t_norm.forward(i_t)\n",
    "t_dout = torch.randn(B,T,C)\n",
    "t_loss = (t_out * t_dout).sum()  # the gradient of sum equals the cp.ones((5,))\n",
    "#t_grads = cp.array(torch.autograd.grad(outputs=[torch_sum],inputs=[i_t])[0].numpy())\n",
    "#print(t_grads.shape,t_grads.mean())\n",
    "#print(t_grads.squeeze())\n",
    "t_loss.backward()\n",
    "i_t.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4) 3.793262000802618e-16\n",
      "[[[ 2.96276208 -0.38686635  0.07836982 -2.65426555]\n",
      "  [ 1.35530748  0.32002407 -2.06121654  0.385885  ]\n",
      "  [ 7.05981306  4.30145389 -6.23514508 -5.12612187]]\n",
      "\n",
      " [[ 0.65259884  0.5359905  -0.09594157 -1.09264777]\n",
      "  [ 3.60401811  3.0735491   0.15936946 -6.83693668]\n",
      "  [-4.99137212  4.22525101  3.96801315 -3.20189203]]]\n"
     ]
    }
   ],
   "source": [
    "s_norm = LayerNorm((C,),weight_init=cp.array(t_norm.weight.detach().numpy()),bias_init=cp.array(t_norm.bias.detach().numpy()))\n",
    "c_np = cp.array(i_np,dtype=cp.float64)\n",
    "c_result = s_norm.forward(c_np)\n",
    "upstr = cp.ones((B,T,C))\n",
    "c_grads = s_norm.backward(cp.array(t_dout.detach().numpy()))\n",
    "print(c_grads.shape,c_grads.mean())\n",
    "print(c_grads.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5415, dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "-2.609024107869118e-15\n"
     ]
    }
   ],
   "source": [
    "print(torch_sum)\n",
    "print(c_result.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 2)\n",
      "[[[1 2]\n",
      "  [2 3]\n",
      "  [3 4]]\n",
      "\n",
      " [[4 5]\n",
      "  [5 6]\n",
      "  [6 7]]]\n",
      "(2, 3, 2, 2)\n",
      "[[[[ 1  2]\n",
      "   [ 2  4]]\n",
      "\n",
      "  [[ 4  6]\n",
      "   [ 6  9]]\n",
      "\n",
      "  [[ 9 12]\n",
      "   [12 16]]]\n",
      "\n",
      "\n",
      " [[[16 20]\n",
      "   [20 25]]\n",
      "\n",
      "  [[25 30]\n",
      "   [30 36]]\n",
      "\n",
      "  [[36 42]\n",
      "   [42 49]]]]\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# tst\n",
    "init = np.array([[[1, 2], [2, 3], [3, 4]],\n",
    "            [[4, 5], [5, 6], [6, 7]]])  # Shape (2, 3,2)\n",
    "print(init.shape)\n",
    "print(init)\n",
    "result = np.einsum(\"...i,...j->...ij\", init, init)\n",
    "print(result.shape)\n",
    "print(result) # is the same as result.transpose(0,1,3,2) due to the symerty in the last dimension\n",
    "a=0\n",
    "b=1\n",
    "print(result[(a,b,0,1)])  # == result[a,b,0] * result[a,b,1]\n",
    "print( 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2, 3, 3)\n",
      "(4, 2, 3, 1)\n",
      "(4, 2, 3, 1)\n",
      "[[4610]\n",
      " [4826]\n",
      " [5042]]\n",
      "4610\n"
     ]
    }
   ],
   "source": [
    "dima = (4,2,3,3)\n",
    "A = np.arange(18*4).reshape(dima)\n",
    "B = np.arange(2,26).reshape((4,2,3,1))\n",
    "print(A.shape)\n",
    "print(B.shape)\n",
    "res= np.matmul(A,B)\n",
    "print(res.shape)\n",
    "print(res[(3,1)])\n",
    "manual = A[(3,1,0,0)] * B[(3,1,0,0)] + A[(3,1,0,1)] * B[(3,1,1,0)] +A[(3,1,0,2)] * B[(3,1,2,0)]\n",
    "print(manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3019, dtype=torch.float64)\n",
      "1.3019223291148998\n"
     ]
    }
   ],
   "source": [
    "# test crossentropy\n",
    "V = 5\n",
    "B ,T,C = 2,3,4\n",
    "logits = cp.random.random((B,T,C)).reshape((B*T,C))\n",
    "def one_hot(a, num_classes):\n",
    "    return cp.squeeze(cp.eye(num_classes)[a.reshape(-1)])\n",
    "targets_raw = cp.array([[0,1,0],[1,3,0]])\n",
    "targets =torch.tensor(targets_raw.flatten())\n",
    "res = torch.nn.functional.cross_entropy(torch.tensor(logits),targets)\n",
    "print(res.cpu())\n",
    "\n",
    "eps = 1e-6\n",
    "def cross_entropy_loss(y_pred: ArrayLike, y_true: ArrayLike) -> cp.ndarray:\n",
    "    \"\"\"\n",
    "    Compute cross entropy loss between true 1-hot encoded vector and softmax output of a predictor.\n",
    "    \"\"\"\n",
    "    from layers import Softmax\n",
    "    # Make sure to not have log(0)\n",
    "    y_pred = cp.clip(y_pred, eps, 1 - eps)\n",
    "    # Compute cross entropy loss\n",
    "    sm = Softmax(axis=-1)\n",
    "    outputs = sm.forward(y_pred)\n",
    "\n",
    "    loss = -cp.sum(y_true * cp.log(outputs + 1e-9)) / y_true.shape[0]\n",
    "    return loss\n",
    "\n",
    "print(cross_entropy_loss(logits,one_hot(targets_raw,4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
